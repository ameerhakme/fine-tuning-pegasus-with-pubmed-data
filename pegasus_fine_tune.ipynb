{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16631a1",
   "metadata": {},
   "source": [
    "# Hugging Face and Sagemaker: fine-tuning Pegasus with MEDLINE/PubMed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e66aa",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7423739",
   "metadata": {},
   "source": [
    "In this demo, we will use the [Hugging Face transformers](https://huggingface.co/docs/transformers/index) library to fine-tune the Pegasus model on the Medline PubMed dataset for text summarization tasks. We will then evaluate the performance of the resulting model using various metrics and techniques. Finally, we will deploy the model for inference on a [SageMaker](https://aws.amazon.com/sagemaker/) Endpoint, allowing us to generate text summaries quickly and efficiently. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e80c86",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65958c43",
   "metadata": {},
   "source": [
    "[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus) is a transformer-based model that was introduced by Google AI in 2020. It is specifically designed for abstractive text summarization tasks and has shown impressive results in various benchmark datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375fb3a3",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d715eb",
   "metadata": {},
   "source": [
    "The [Medline PubMed](https://huggingface.co/datasets/scientific_papers/viewer/pubmed/train) dataset is a widely-used collection of scientific research articles in the field of biomedical sciences. It contains millions of abstracts and citations from various research journals and publications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976aaa34",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de5fc9",
   "metadata": {},
   "source": [
    "[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus) is a transformer-based model that was introduced by Google AI in 2020. It is specifically designed for abstractive text summarization tasks and has shown impressive results in various benchmark datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbbfa60",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e6feb",
   "metadata": {},
   "source": [
    "First, you need to install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd11e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers --quiet\n",
    "%pip install nltk --quiet\n",
    "%pip install accelerate --quiet\n",
    "%pip install datasets --quiet\n",
    "%pip install rouge_score --quiet\n",
    "%pip install evaluate --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408437c",
   "metadata": {},
   "source": [
    "### Variables and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c4271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# vars\n",
    "model_checkpoint = 'google/pegasus-xsum'\n",
    "bucket_name = 'pegsum-content-bucket'\n",
    "artifact_path = 'training_artifacts/%s/' % datetime.today().strftime('%Y-%m-%d') \n",
    "\n",
    "# tokenizer\n",
    "max_target_length = 32\n",
    "max_input_length = 512\n",
    "ds_col_full = \"article\"\n",
    "ds_col_summ = \"abstract\"\n",
    "\n",
    "# training\n",
    "batch_size = 1\n",
    "num_train_epochs = 5\n",
    "learning_rate = .001\n",
    "optimizer_name = 'Adam' # must be a supported algorithm from https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d6d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7765bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"scientific_papers\", \"pubmed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed13d64",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Prepares data for the model by mapping text into numerical inputs called tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50819eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[ds_col_full],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[ds_col_summ], max_length=max_target_length, truncation=True, padding='max_length'\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6089084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f4516",
   "metadata": {},
   "source": [
    "### Data Collator\n",
    "Pads data during batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb15232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb17cc3",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "Incrementally loads data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa5186",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "The optimizer maintains training state and update parameters based on training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd688874",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hardcode the optimizer, replaced by following code block\n",
    "\n",
    "#from torch.optim import Adam\n",
    "\n",
    "#optimizer = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically select optimizer based on input var\n",
    "\n",
    "from importlib import import_module\n",
    "\n",
    "module = import_module('torch.optim')\n",
    "opt_fnc = getattr(module, optimizer_name)\n",
    "\n",
    "optimizer = opt_fnc(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54032d51",
   "metadata": {},
   "source": [
    "### Accelerator\n",
    "The accelerator enables distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d9f82",
   "metadata": {},
   "source": [
    "### Learning rate scheduler\n",
    "Manages adjustments to the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aafcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c79cb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "epoch_scores = []\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            labels = accelerator.pad_across_processes(\n",
    "                batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "            # Replace -100 in the labels as we can't decode them\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True\n",
    "            )\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            decoded_preds, decoded_labels = postprocess_text(\n",
    "                decoded_preds, decoded_labels\n",
    "            )\n",
    "\n",
    "            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    result = rouge_score.compute()\n",
    "    # Extract the median ROUGE scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    \n",
    "    # Save epoch score\n",
    "    epoch_score = \"Epoch %s: %s\" % (epoch, result)\n",
    "    epoch_scores.append(epoch_score)\n",
    "    print(epoch_score)\n",
    "\n",
    "    # Save model for this epoch\n",
    "    epoch_name = \"epoch_%s/\" % epoch  \n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained('model_dir', save_function=accelerator.save)\n",
    "    \n",
    "    # Upload epoch artifacts to S3\n",
    "    with open(\"model_dir/pytorch_model.bin\", \"rb\") as f:\n",
    "        s3.upload_fileobj(f, bucket_name, artifact_path + epoch_name + \"pytorch_model.bin\")\n",
    "    with open(\"model_dir/generation_config.json\", \"rb\") as f:\n",
    "        s3.upload_fileobj(f, bucket_name, artifact_path + epoch_name + \"generation_config.json\")\n",
    "    with open(\"model_dir/config.json\", \"rb\") as f:\n",
    "        s3.upload_fileobj(f, bucket_name, artifact_path + epoch_name + \"config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0becb671",
   "metadata": {},
   "source": [
    "### Write each epoch's rouge scores to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfbbaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"epoch_scores.txt\", \"w\") as f:\n",
    "    for entry in epoch_scores:\n",
    "        f.write(entry + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16758e1",
   "metadata": {},
   "source": [
    "### Save scores and tokenizer to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"epoch_scores.txt\", \"rb\") as f:\n",
    "    s3.upload_fileobj(f, bucket_name, artifact_path + \"epoch_scores.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('model_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5acc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_dir/special_tokens_map.json\", \"rb\") as f:\n",
    "    s3.upload_fileobj(f, bucket_name, artifact_path + \"special_tokens_map.json\")\n",
    "with open(\"model_dir/tokenizer_config.json\", \"rb\") as f:\n",
    "    s3.upload_fileobj(f, bucket_name, artifact_path + \"tokenizer_config.json\")\n",
    "with open(\"model_dir/tokenizer.json\", \"rb\") as f:\n",
    "    s3.upload_fileobj(f, bucket_name, artifact_path + \"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332b0a8",
   "metadata": {},
   "source": [
    "### Zip and save the model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd model_dir/\n",
    "!tar -czvf model.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.tar.gz\", \"rb\") as f:\n",
    "    s3.upload_fileobj(f, bucket_name, artifact_path + \"model/model.tar.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
